"""
Script ƒë·ªÉ ki·ªÉm tra v√† xem d·ªØ li·ªáu trong database
"""
import sys
from pathlib import Path

project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.qa_communicate.database.database import get_db
from src.qa_communicate.database.repository import EvaluationRepository, SegmentRepository
from tabulate import tabulate  # pip install tabulate


def show_statistics():
    """Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng quan"""
    print("\n" + "="*80)
    print("üìä TH·ªêNG K√ä T·ªîNG QUAN")
    print("="*80)
    
    with get_db() as db:
        stats = EvaluationRepository.get_statistics(db)
        
        print(f"\nüìà T·ªïng s·ªë evaluations: {stats['total_evaluations']}")
        print(f"   ‚îú‚îÄ ‚è≥ Pending: {stats['pending']}")
        print(f"   ‚îú‚îÄ üîÑ Processing: {stats['processing']}")
        print(f"   ‚îú‚îÄ ‚úÖ Completed: {stats['completed']}")
        print(f"   ‚îî‚îÄ ‚ùå Failed: {stats['failed']}")
        
        if stats['average_score'] is not None:
            print(f"\nüíØ ƒêi·ªÉm trung b√¨nh: {stats['average_score']}/2.0")
        
        if stats['error_distribution']:
            print(f"\n‚ö†Ô∏è  Ph√¢n b·ªë m·ª©c l·ªói:")
            for level, count in stats['error_distribution'].items():
                print(f"   ‚îú‚îÄ {level}: {count} cu·ªôc g·ªçi")


def show_recent_evaluations(limit=10):
    """Hi·ªÉn th·ªã c√°c evaluations g·∫ßn nh·∫•t"""
    print("\n" + "="*80)
    print(f"üìã {limit} CU·ªòC G·ªåI G·∫¶N NH·∫§T")
    print("="*80)
    
    with get_db() as db:
        evaluations = EvaluationRepository.list_all(db, limit=limit)
        
        if not evaluations:
            print("\n‚ö†Ô∏è  Ch∆∞a c√≥ d·ªØ li·ªáu n√†o trong database.")
            return
        
        table_data = []
        for eval in evaluations:
            table_data.append([
                eval.task_id[:12] + "...",
                eval.filename[:30] if eval.filename else "N/A",
                eval.status,
                f"{eval.tong_diem:.1f}" if eval.tong_diem is not None else "N/A",
                eval.muc_loi or "N/A",
                eval.created_at.strftime("%Y-%m-%d %H:%M:%S")
            ])
        
        headers = ["Task ID", "Filename", "Status", "ƒêi·ªÉm", "M·ª©c l·ªói", "Th·ªùi gian"]
        print("\n" + tabulate(table_data, headers=headers, tablefmt="grid"))


def show_evaluation_detail(task_id: str):
    """Hi·ªÉn th·ªã chi ti·∫øt m·ªôt evaluation"""
    print("\n" + "="*80)
    print(f"üîç CHI TI·∫æT EVALUATION: {task_id}")
    print("="*80)
    
    with get_db() as db:
        evaluation = EvaluationRepository.get_by_task_id(db, task_id)
        
        if not evaluation:
            print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y evaluation v·ªõi task_id: {task_id}")
            return
        
        print(f"\nüìÅ File Info:")
        print(f"   ‚îú‚îÄ Filename: {evaluation.filename}")
        print(f"   ‚îú‚îÄ Size: {evaluation.file_size_mb:.2f} MB")
        print(f"   ‚îî‚îÄ Status: {evaluation.status}")
        
        print(f"\n‚è∞ Timestamps:")
        print(f"   ‚îú‚îÄ Created: {evaluation.created_at}")
        print(f"   ‚îî‚îÄ Completed: {evaluation.completed_at or 'N/A'}")
        
        if evaluation.status == 'completed':
            print(f"\nüìä Call Metadata:")
            print(f"   ‚îú‚îÄ Duration: {evaluation.duration:.2f}s")
            print(f"   ‚îú‚îÄ Turns: {evaluation.turns}")
            print(f"   ‚îî‚îÄ Sales Ratio: {evaluation.ratio_sales:.2%}")
            
            print(f"\nüíØ Scoring Results:")
            print(f"   ‚îú‚îÄ Ch√†o/X∆∞ng danh: {evaluation.chao_xung_danh}/1")
            print(f"   ‚îú‚îÄ K·ªπ nƒÉng n√≥i: {evaluation.ky_nang_noi}/1")
            print(f"   ‚îú‚îÄ K·ªπ nƒÉng nghe: {evaluation.ky_nang_nghe}/1")
            print(f"   ‚îú‚îÄ Th√°i ƒë·ªô: {evaluation.thai_do}/1")
            print(f"   ‚îú‚îÄ üéØ T·ªïng ƒëi·ªÉm: {evaluation.tong_diem:.2f}/2.0")
            print(f"   ‚îî‚îÄ ‚ö†Ô∏è  M·ª©c l·ªói: {evaluation.muc_loi}")
            
            print(f"\nüìù L√Ω do chi ti·∫øt:")
            print(f"   {evaluation.ly_do}")
            
            # Show segments
            segments = SegmentRepository.get_by_evaluation_id(db, evaluation.id)
            if segments:
                print(f"\nüó£Ô∏è  Segments ({len(segments)} ƒëo·∫°n):")
                seg_table = []
                for seg in segments[:5]:  # Show first 5 segments
                    seg_table.append([
                        seg.segment_number,
                        seg.speaker,
                        f"{seg.start_time:.1f}s - {seg.end_time:.1f}s",
                        seg.text[:50] + "..." if seg.text and len(seg.text) > 50 else seg.text,
                        f"{seg.speed_spm:.0f}" if seg.speed_spm else "N/A"
                    ])
                
                headers = ["#", "Speaker", "Time", "Text", "SPM"]
                print(tabulate(seg_table, headers=headers, tablefmt="grid"))
                
                if len(segments) > 5:
                    print(f"   ... v√† {len(segments) - 5} segments kh√°c")
        
        elif evaluation.status == 'failed':
            print(f"\n‚ùå Error Message:")
            print(f"   {evaluation.error_message}")


def show_error_distribution():
    """Hi·ªÉn th·ªã ph√¢n b·ªë l·ªói chi ti·∫øt"""
    print("\n" + "="*80)
    print("‚ö†Ô∏è  PH√ÇN B·ªê M·ª®C L·ªñI CHI TI·∫æT")
    print("="*80)
    
    with get_db() as db:
        from sqlalchemy import func
        from src.qa_communicate.database.models import Evaluation
        
        results = db.query(
            Evaluation.muc_loi,
            func.count(Evaluation.id).label('count'),
            func.avg(Evaluation.tong_diem).label('avg_score')
        ).filter(
            Evaluation.status == 'completed',
            Evaluation.muc_loi.isnot(None)
        ).group_by(Evaluation.muc_loi).all()
        
        if not results:
            print("\n‚ö†Ô∏è  Ch∆∞a c√≥ d·ªØ li·ªáu completed ƒë·ªÉ ph√¢n t√≠ch.")
            return
        
        table_data = []
        for muc_loi, count, avg_score in results:
            table_data.append([
                muc_loi,
                count,
                f"{avg_score:.2f}" if avg_score else "N/A"
            ])
        
        headers = ["M·ª©c l·ªói", "S·ªë l∆∞·ª£ng", "ƒêi·ªÉm TB"]
        print("\n" + tabulate(table_data, headers=headers, tablefmt="grid"))


def interactive_menu():
    """Menu t∆∞∆°ng t√°c"""
    while True:
        print("\n" + "="*80)
        print("üóÑÔ∏è  DATABASE INSPECTOR")
        print("="*80)
        print("\n[1] Xem th·ªëng k√™ t·ªïng quan")
        print("[2] Xem 10 cu·ªôc g·ªçi g·∫ßn nh·∫•t")
        print("[3] Xem chi ti·∫øt m·ªôt evaluation (theo task_id)")
        print("[4] Xem ph√¢n b·ªë m·ª©c l·ªói")
        print("[5] Xem t·∫•t c·∫£ evaluations (c√≥ pagination)")
        print("[0] Tho√°t")
        
        choice = input("\nCh·ªçn (0-5): ").strip()
        
        if choice == '0':
            print("\nüëã T·∫°m bi·ªát!")
            break
        elif choice == '1':
            show_statistics()
        elif choice == '2':
            show_recent_evaluations(10)
        elif choice == '3':
            task_id = input("\nNh·∫≠p task_id: ").strip()
            if task_id:
                show_evaluation_detail(task_id)
            else:
                print("‚ùå Task ID kh√¥ng h·ª£p l·ªá!")
        elif choice == '4':
            show_error_distribution()
        elif choice == '5':
            limit = input("\nS·ªë l∆∞·ª£ng records (m·∫∑c ƒë·ªãnh 20): ").strip()
            limit = int(limit) if limit.isdigit() else 20
            show_recent_evaluations(limit)
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
        
        input("\nNh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")


if __name__ == "__main__":
    try:
        interactive_menu()
    except KeyboardInterrupt:
        print("\n\nüëã ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh!")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()